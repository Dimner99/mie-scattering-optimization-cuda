# Numerical Computation of Bessel Functions $J_n$ and $Y_n$ via Recurrence Relations

The entire computation of Bessel functions in this project rests on a single identity — the standard three-term recurrence relation shared by both $J_n(z)$ and $Y_n(z)$:

$$f_{n-1}(z) + f_{n+1}(z) = \frac{2n}{z} f_n(z)$$

Because this is a second-order linear difference equation, its general solution is always a linear combination of the two independent Bessel solutions, $f_n = C_1 J_n(z) + C_2 Y_n(z)$, where $C_1$ and $C_2$ are fixed by whichever two values we choose as seeds. The key insight is that although $J_n$ and $Y_n$ satisfy exactly the same recurrence, their asymptotic behavior for large order $n$ is wildly different, and this asymmetry dictates which direction — forward or backward — we may safely iterate.

To see why, consider what happens as $n \gg z$. Stirling's approximation gives $J_n(z) \approx \frac{1}{\sqrt{2\pi n}} \left( \frac{e z}{2n} \right)^n$, which decays factorially toward zero, while $Y_n(z) \approx -\sqrt{\frac{2}{\pi n}} \left( \frac{2n}{e z} \right)^n$, which explodes factorially toward $-\infty$. In other words, $|Y_n| \gg |J_n|$ for large $n$, with $Y_n$ roughly the reciprocal of $J_n$ in magnitude.

This disparity is precisely what makes backward recurrence the correct strategy for computing $J_n$ (Miller's algorithm). Suppose we start at some large $N$ with an arbitrary guess — say $f_{N+1} = 0$, $f_N = 10^{-30}$. That guess inevitably contains some admixture of both solutions, $f_N = C_1 J_N + C_2 Y_N$. As we recurse backwards toward $n = 0$, each step multiplies the ratio of the unwanted $Y$ component to the wanted $J$ component by approximately $\left( \frac{e z}{2n} \right)^2$, a factor much smaller than one since $N \gg z$. By the time we reach $n = 0$, the $Y_n$ contamination has been suppressed by a massive factor on the order of $(z / 2N)^{2N}$, leaving $f_n$ effectively proportional to pure $J_n(z)$. The proportionality constant $A$ is then determined by normalizing against a known reference value: $A = f_0 / J_0^{\text{true}}(z)$, after which $J_n(z) = f_n / A$. This is the complete Miller's algorithm — an arbitrary high-order seed, a backward sweep of the recurrence, and a single normalization step.

Forward recurrence, by contrast, is catastrophic for $J_n$. Even if we start with perfect seeds $f_0 = J_0$, $f_1 = J_1$, any floating-point rounding error $\epsilon$ introduces a tiny $Y_n$ component, so $f_n = J_n + \epsilon\, Y_n$. As we iterate forward, $Y_n$ grows factorially while $J_n$ decays, driving $\frac{\epsilon\, Y_n}{J_n} \to \infty$ — the error overwhelms the signal within a few steps. However, this very instability becomes an advantage when the goal is to compute $Y_n$ itself. Any parasitic $J_n$ component decays to zero on its own, while the dominant $Y_n$ signal only grows stronger, making forward recurrence inherently stable for the Neumann function. The only requirement is that we supply exact seeds $Y_0(x)$ and $Y_1(x)$ (computed, in our case, via SciPy on the CPU), then iterate $Y_{n+1} = \frac{2n}{x}\,Y_n - Y_{n-1}$ upward. No normalization is needed because the seeds were already exact, and the recurrence stays locked onto the dominant branch.

In summary, the algorithm splits into two complementary passes. For $J_n(z)$ we run Miller's backward recurrence: seed at large $N$ with $f_{N+1}=0$, $f_N=10^{-30}$; sweep $f_{n-1} = \frac{2n}{z}\,f_n - f_{n+1}$ down to $n=0$; and normalize by dividing through by $f_0 / J_0(z)$. For $Y_n(x)$ we run forward recurrence: seed with exact $Y_0(x)$ and $Y_1(x)$; sweep $Y_{n+1} = \frac{2n}{x}\,Y_n - Y_{n-1}$ up to the desired order; and use the result directly. Together, these two passes yield the full set of $J_n$ and $Y_n$ values needed to construct the Hankel function $H_n^{(2)}(x) = J_n(x) - i\,Y_n(x)$ and, from there, the Mie scattering coefficients.